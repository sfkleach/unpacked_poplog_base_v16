% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%      Extracts from the book "Natural Language Processing in LISP"     %
%                      published by Addison Wesley                      %
%        Copyright (c) 1989, Gerald Gazdar & Christopher Mellish.       %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

Bottom-up parsing

We can write a very naive bottom-up parser quite easily in LISP (lib
burecog).  First of all, let us assume that we are provided with the
rules of Grammar1, represented in LISP and stored in the global
variable rules, as in Chapter 4.

As in the previous discussion, we will present a recognizer rather
than a parser, because this is simpler and the changes that make this
into a parser are not great.  We can think of determining whether a
given string is grammatical as being the task of finding a sequence of
rewrites that starts with the string and ends up with the symbol S,
for instance:

    (MediCenter employed nurses)
    (MediCenter (V) nurses)
    (MediCenter (V) (NP))
    (MediCenter (VP))
    ((NP) (VP))
    ((S))

Each of these rewriting steps must be allowed by some rule of the
grammar.  For instance, the very last step is justified by the first
rule saying that a sentence can be a NP followed by a VP.  Notice that
we have put grammatical categories inside lists to distinguish them
from words.  This is not necessary for this program, but it paves the
way for programs that use more complex category descriptions.  The
function next, defined below, takes a list (representing a string of
words and/or categories) as its argument and works out all possible
ways of performing a single rewrite, using the rules of the grammar.
Having obtained one of these rewrites, it calls next recursively on
the new list.  This next next finds out all possible rewrites of this,
calling next on them, and so on.  The original call of next, the calls
of next generated by that, the calls of next generated by them, and so
on thus cause the whole search space of possible sequences of rewrites
to be explored.

Here now is a possible definition for next:

    (defun next (string)
      (if (equal string '((S)))
        (print '(yes))
        (do
          (
           (left nil (append left (list (car tape))))
           (tape string (cdr tape)))
          ((null tape))      ; do until end of tape
          (dolist (rule rules)
            (let ((newstring (rewrite tape (car rule) (cdr rule))))
              (if (null newstring)
                nil    ; rewrite failed
                (next (append left newstring))))))))

next finds all possible rewrites of its argument by working its way
through the list from left to right.  At any time, it keeps the list
of items it has already worked through in the variable left and the
list it still has to consider in tape.  If it can find a rule in the
grammar such that an initial portion of tape matches the body of the
rule (using rewrite), then it can successfully produce a rewritten
list and call next with it.  The recursive next call is given a list
consisting of all the elements to the left of the matched part,
followed by the head of the grammar rule, followed by all the elements
to the right of the matched part.  The function rewrite is given the
portion of the string after left and the LHS and RHS of a rule.  It
sees whether the elements of the RHS match the elements at the start
of the string, and if so returns a new version of the string with the
LHS substituted for the elements that the RHS matched:

    (rewrite '((NP)(VP)(PP)) '(S) '((NP)(VP)))
    ((S) (PP))

Here is its definition:

    (defun rewrite (string LHS RHS)
      (if (null RHS)      ; successful rewrite
        (cons LHS string)
        (if (null string) ; end of string
          nil
          (if (equal (car string) (car RHS))
            (rewrite (cdr string) LHS (cdr RHS))
            nil)))) ; rule does not match

If we call next on a given list of atoms, when does the program
terminate?  The program only terminates when it has explored the whole
search space, that is, explored all possible ways in which the string
could be a sentence.  If there is at least one possible way, then,
somewhere deep in the tree, a call of next will be generated with
((S)) as its argument.  This call will print out the message (yes) and                     
will then terminate without calling any more nexts.  Of course, other
nexts will in general have to run before the search is complete and
the program as a whole terminates.

To make our recognizer into a parser, all we need to do is enhance our
representation of the sequence of words and categories to include
parse trees instead of categories.  Thus our successful rewriting
sequence will now look as follows:

    (MediCenter employed nurses)
    (MediCenter (V employed) nurses)
    (MediCenter (V employed) (NP nurses))
    (MediCenter (VP (V employed) (NP nurses)))
    ((NP MediCenter) (VP (V employed) (NP nurses)))
    ((S (NP MediCenter) (VP (V employed) (NP nurses))))

To implement this, we need to introduce a couple of changes in the
next function.  First of all, the termination condition needs to be
changed, so that any tree labelled by an S is displayed when it
appears as the only element in the list.  In fact, we can generalize
this to allow for any single tree to be accepted as a solution:

    (if (equal (length string) 1)
     ...

Secondly, when next is called recursively, the new list should not
contain just the LHS of the rule, but should contain the category
label of the head followed by the subtrees that have been found in the
old list.  Moreover, if the elements stored in the old list are parse
trees, rather than simple categories, matching the sequence of
elements with the body of a rule (where the elements are simple
categories) is not so straightforward.  We thus require the actions
done for each rule in the grammar to be replaced by:

            (let ((needed_others (initial_segment (cdr rule) tape)))
              (if (null needed_others)
                nil  ; rewrite failed
                (next (append left (list (cons (caar rule) (car needed_others))) (cadr needed_others)))

where initial_segment is a function to be defined.  initial_segment
takes as its first argument the RHS of a rule - an arbitrary list of
individual atoms and category labels in lists, e.g. ((NP) and (NP)).
As its second argument it takes a sequence of items that could appear
in the input list for next - an arbitrary list of individual words and
parse trees (represented as lists), e.g. ((NP Dr Chan) and (NP nurses)
(V died)).  It decomposes its second argument into two sublists and
returns a list containing these as its result.  The first sublist is
the initial portion of the list which 'matched' against the rule body,
and the second sublist is the rest of the list.  For instance:

    (initial_segment '((NP) and (NP))
       '((NP Dr Chan) and (NP nurses) (V died)))

    (((NP DR CHAN) AND (NP NURSES)) ((V DIED)))

Top-down parsing

We can write a top down recognizer similar to our bottom up
recognizer.  Whereas a bottom up recognizer needs to know only about
what it has successfully found at any point, a top down recognizer
needs to keep track of what it is trying to find - its goals.  So our
top down version of next will take a first argument which is the list
of categories (represented by names inside lists) and words
(represented by LISP atoms) that it is trying to find, in the order it
is to find them.  When we call next originally, this list will simply
be ((S)), indicating that the recognizer just needs to find an (S).
The second argument to next will be the input string (list of atoms)
which is to provide the words to satisfy these goals.  In the bottom
up case we could think of a recognition as a sequence of rewrites of
the original string resulting in ((S)), but we must now think of                     
generating sequences of goals-string pairs, with success indicated by
both becoming empty:

    goals                string

    ((S))                (MediCenter employed nurses)
    ((NP) (VP))          (MediCenter employed nurses)
    (MediCenter (VP))    (MediCenter employed nurses)
    ((VP))               (employed nurses)
    ((V) (NP))           (employed nurses)
    (employed (NP))      (employed nurses)
    ((NP))               (nurses)
    (nurses)             (nurses)
    ()                   ()

As before, each step in a successful sequence must be justified.  If
the first item in the goal list is a category, one is justified in
proceeding to a pair where the string is unaltered but the first goal
has been replaced by the right hand side of a rule whose head matches
it.  If the first item in the goal list is a word which is the same as
the first word in the string, one is justified in proceeding to a pair
where the two identical words have been deleted.

So much for the legal moves.  Our approach to searching through all
possibilities (in lib tdrecog) will be as in the previous program -
given a goals-string pair, next works out all the pairs that could
immediately follow and calls next recursively on them.  Thus once
again the tree of next calls will mirror the search space exactly.
Here is our top down version of next:

    (defun next (goals string)
      (if (and (null goals) (null string))
        (print '(yes))
        (if (listp (car goals))
          (dolist (rule rules)
            (if (equal (car goals) (car rule))
              (next (append (cdr rule) (cdr goals)) string)))
          (if (equal (car goals) (car string))
            (next (cdr goals) (cdr string))))))

If you watch this program at work, a certain lack of intelligence
manifests itself.  For instance, from the state:

    goals                     string

    ((V) (NP))                (believed nurses)

it generates all of:

    (died (NP))               (believed nurses)
    (employed (NP))           (believed nurses)
    (approved (NP))           (believed nurses)
    (appeared (NP))           (believed nurses)
    (believed (NP))           (believed nurses)

that is, one state for every possible verb.  If there were 500 known
verbs, this would lead to 500 states to explore, even though only one
of them could be equal to the next word in the string.  This is indeed
what a pure top down analyser would do.  A more sensible move would be
to use lexical rules bottom up but other grammar rules top down.

Breadth first and depth first      

Since the digital computers that we commonly use cannot do more than
one thing at a time, when we implement a breadth first search
algorithm we need to introduce a device to enable the machine to spend
its time evenly in different parts of the search space.  We will do
this in a simple breadth first recognizer by collecting all the
alternative possible states into a list.  As this is a bottom up
recognizer, we can summarise all we need to know about a state by the
list of words and categories that we have found so far (as in lib
burecog).  The breadth first program proceeds in a sequence of cycles.
Each cycle involves going through all the alternatives in the list,
working out for each one what new states could follow from it.  These
new states are all collected into a list which forms the list to be
processed in the next cycle.  Here is the breadth first bottom up
recognizer:

    (defun recognize (string)
      (do ((alternatives (list string))) ((null alternatives))
        (setq alternatives (next_states_list alternatives))))

    (defun next_states_list (list)
      (if (null list)
        '()
        (append
          (next_states (car list))
          (next_states_list (cdr list)))))

    (defun next_states (string)
      (if (equal (length string) 1)
        (print (car string)))
      (do (
         (left () (append left (list (car remaining))))
         (remaining string (cdr remaining))
         (results ()))
        ((null remaining) results)      ; do until end of string
        (dolist (rule rules)
          (let ((newstring (rewrite remaining (car rule) (cdr rule))))
            (if newstring
              (setq results (cons (append left newstring) results)))))))

The function recognize is to be called with a list of words as its
argument.  The list of alternative states to explore from is kept in
the variable alternatives.  Each cycle involves going through the
alternatives, computing next states.  These are all collected up,
using next_states_list, to form the new alternatives.  The core of the
function, matching parts of the string with the right hand sides of
rules (function rewrite), is as in our depth first bottom up
recognizer.  next_states_list is given a list of search states and
returns the result of appending together the next states from all the
states in that list.  The crucial function is, however, next_states,
which returns the list of next states that follow from a given state.
This function works its way down the string, keeping the remaining
elements in the list remaining and the elements already encountered in
left.  It tries applying each possible rule at each position in the
string (remaining holding the part of the string whose beginning will
be matched against rule RHSs).
