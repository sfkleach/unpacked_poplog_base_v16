% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%     Extracts from the book "Natural Language Processing in POP-11"    %
%                      published by Addison Wesley                      %
%        Copyright (c) 1989, Gerald Gazdar & Christopher Mellish.       %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

Bottom-up parsing

We can write a very naive bottom-up parser quite easily in POP-11 (lib
burecog).  First of all, let us assume that we are provided with the
rules of Grammar1, represented in POP-11 in the global variable RULES
as in Chapter 4.  As in the previous discussion, we will present a
recognizer rather than a parser, because this is simpler and the
changes that are required to make this into a parser are not great.
We can think of determining whether a given string is grammatical as
being the task of finding a sequence of rewrites that starts with the
string and ends up with the symbol S, for instance:

    [MediCentre employed nurses]
    [MediCentre [V] nurses]
    [MediCentre [V] [NP]]
    [MediCentre [VP]]
    [[NP] [VP]]
    [[S]]

Each of these rewriting steps must be allowed by some rule of the
grammar.  For instance, the very last step is justified by the first
rule saying that a sentence can be a noun phrase followed by a verb
phrase.  Notice that we have put grammatical categories inside lists
to distinguish them from words.  This is not necessary for this
program, but it paves the way for programs that use more complex
category descriptions.  The procedure next, defined later, takes a
list (representing a string of words and/or categories) as its
argument and works out all possible ways of performing a single
rewrite, using the rules of the grammar.  Having obtained one of these
rewrites, it calls next recursively on the new list.  This next next
finds out all possible rewrites of this, calling next on them, and so
on.  The original call of next, the calls of next generated by that,
the calls of next generated by them, and so on thus cause the whole
search space of possible sequences of rewrites to be explored.
Notice that the tree of calls mirrors exactly the search tree.  Here                     
now is a possible definition for NEXT:

    define next(string);
       vars left rule head body this right others;
       if string matches [[S]] then
          [yes] =>
       else
          [] -> left;
          while string matches [?this ??right] do
             for rule in rules do
                rule --> [?head ??body];
                if string matches [^^body ??others] then
                   next([^^left ^head ^^others])
                endif
             endfor;
             [^^left ^this] -> left;
             right -> string
          endwhile
       endif
    enddefine;

Next finds all possible rewrites of its argument by working its way
through the list from left to right.  At any time, it keeps the list
of items it has already worked through in the variable left and the
list it still has to consider in string.  If it can find a rule in the
grammar such that an initial portion of string matches the body of the
rule, then it can successfully produce a rewritten list and call next
with it.  The recursive next call is given a list consisting of all
the elements to the left of the matched part, followed by the head of
the grammar rule, followed by all the elements to the right of the
matched part (others).

If we call next on a given list of words, when does the program
terminate?  The program only terminates when it has explored the whole
search space; that is, when it has explored all possible ways in which
the string could be a sentence.  If there is at least one possible
way, then, somewhere deep in the tree, a call of next will be
generated with [[S]] as its argument.  This call will print out the
message [yes] and will then terminate without any more calls on next.
Of course, other calls on next will, in general, have to run before
the search is complete and the program as a whole terminates.

To make our recognizer into a parser, all we need to do is to enhance
our representation of the sequence of words and categories to include
parse trees instead of categories.  Thus, our successful rewriting
sequence will now look as follows:

    [MediCentre employed nurses]
    [MediCentre [V employed] nurses]
    [MediCentre [V employed] [NP nurses]]
    [MediCentre [VP [V employed] [NP nurses]]]
    [[NP MediCentre] [VP [V employed] [NP nurses]]]
    [[S [NP MediCentre] [VP [V employed] [NP nurses]]]]

To implement this, we need to introduce a couple of changes in the
next procedure.  First of all, the termination condition needs to be
changed, so that any tree labelled by an S is displayed when it
appears as the only element in the list.  In fact, we can generalize
this to allow for any single tree to be accepted as a solution:

    if length(string) = 1 then
       string(1) =>
     ...

Secondly, when next is called recursively, the new list should not
contain just the head of the rule, but should also contain the
category label of the head, followed by the subtrees that have been
found in the old list.  Moreover, if the elements stored in the old
list are parse trees, rather than simple categories, matching the
sequence of elements with the body of a rule (where the elements are
simple categories) is not so straightforward.  We thus require the
actions performed for each rule in the grammar to be replaced by:

                   rule --> [[?head] ??body];
                   initial_segment(body,string) -> needed -> others;
                   if others then
                      next([^^left [^head ^^needed] ^^others])
                   endif

where initial_segment is a procedure to be defined.  initial_segment
takes as its first argument the body (or RHS) of a rule; that is, an
arbitrary list of individual words and category labels in lists, such
as [[NP] and [NP]].  As its second argument, it takes a sequence of
items that could appear in the input list for next; that is, an
arbitrary list of individual words and parse trees (represented as
lists), such as [[NP Dr. Chan] and [NP nurses] [V died]].  It
decomposes its second argument into two sublists, which it returns as
its two results.  The sublist needed is the initial portion of the
list that matches the rule body, while the sublist others is the rest
of the list.  For instance:

    : initial_segment([[NP] and [NP]],
    :                 [[NP Dr. Chan] and [NP nurses] [V died]]) -> needed -> others;
    : needed =>
    ** [[NP Dr. Chan] and [NP nurses]]
    : others =>
    ** [[V died]]

Top-down parsing      

We can write a top-down recognizer similar to our bottom-up
recognizer.  Whereas a bottom-up recognizer needs to know only about
what it has successfully found at any point, a top-down recognizer
also needs to keep track of what it is trying to find - that is, its
goals.  So, our top-down version of next will take a first argument
which is the list of categories (represented by names inside lists)
and words (represented by ordinary POP-11 words) that it is trying to
find, in the order it is to find them.  When we call next originally,
this list will simply be [[S]], indicating that the recognizer just
needs to find an [S].  The second argument to next will be the input
string (list of words) which is to provide the words to satisfy these
goals.  Whereas in the bottom-up case we could think of a recognition
as a sequence of rewrites of the original string resulting in [[S]],
we must now think of generating sequences of goals-string pairs, with
success indicated by both becoming empty:

    Goals                String

    [[S]]                [MediCentre employed nurses]
    [[NP] [VP]]          [MediCentre employed nurses]
    [MediCentre [VP]]    [MediCentre employed nurses]
    [[VP]]               [employed nurses]
    [[V] [NP]]           [employed nurses]
    [employed [NP]]      [employed nurses]
    [[NP]]               [nurses]
    [nurses]             [nurses]
    []                   []

As before, each step in a successful sequence must be justified.  If
the first item in the goal list is a category, we are justified in
proceeding to a pair where the string is unaltered but the first goal
has been replaced by the RHS of a rule whose head matches it.  If the
first item in the goal list is a word that is the same as the first
word in the string, we are justified in proceeding to a pair where the
two identical words have been deleted.

So much for the legal moves.  Our approach to searching through all
possibilities (in lib tdrecog) will be as in the previous program:
given a goals-string pair, next works out all the pairs that could
immediately follow and calls next recursively on them.  Thus, once
again, the tree of next calls will mirror the search space exactly.
Here is our top-down version of next:

    define next(goals, string);
       vars goal restgoals rule subgoals;
       if goals = [] and string = [] then
          [yes] =>
       elseif goals matches [?goal ??restgoals] then
          if islist(goal) then
             for rule in rules do
                if rule matches [^goal ??subgoals] then
                   next([^^subgoals ^^restgoals],string)
                endif
             endfor
          elseif string matches [^goal ??string] then
             next(restgoals, string)
          endif
       endif
    enddefine;

If you watch this program at work, a certain lack of intelligence
manifests itself.  For instance, from the state:

    Goals                     String

    [[V] [NP]]                [believed nurses]

it generates all of the following:

    [died [NP]]               [believed nurses]
    [employed [NP]]           [believed nurses]
    [approved [NP]]           [believed nurses]
    [appeared [NP]]           [believed nurses]
    [believed [NP]]           [believed nurses]

that is, one state for every possible verb.  If there were 500 known
verbs, this would lead to 500 states to explore, even though only one
of them could be equal to the next word in the string.  This is indeed
what a pure top-down analyzer would do.  A more sensible move would be
to use lexical rules bottom up but other grammar rules top down.

Breadth first and depth first      

Since the digital computers that we commonly use cannot do more than
one thing at a time, when we implement a breadth-first search
algorithm we need to introduce a device to enable the machine to spend
its time evenly in different parts of the search space.  We can do
this in a simple breadth-first recognizer by collecting all the
alternative possible states into a list.  As this is a bottom-up
recognizer, we can summarize all we need to know about a state by the
list of words and categories that we have found so far (as in lib
burecog).  The breadth-first program proceeds in a sequence of cycles.
Each cycle involves going through all the alternatives in the list,
working out for each one what new states could follow from it.  These
new states are all collected into a list, which forms the list to be
processed in the next cycle.  Here is the breadth-first bottom-up
recognizer:

    define recognize(string);
       vars alternatives left rule head body this right others;
       [^string] -> alternatives;
       until alternatives = [] do
          [%
             for string in alternatives do
                if length(string) = 1 then
                   string =>
                else
                   [] -> left;
                   while string matches [?this ??right] do
                      for rule in rules do
                         rule --> [?head ??body];
                         if string matches [^^body ??others] then
                            [^^left ^head ^^others]
                         endif
                      endfor;
                      [^^left ^this] -> left;
                      right -> string
                   endwhile
                endif
             endfor
          %] -> alternatives;
      enduntil
    enddefine;

The procedure recognize is to be called with a list of words as its
argument.  The list of alternative states to be explored is kept in
the local variable alternatives.  Each cycle involves going through
the alternatives, computing next states.  These are all collected up,
using the [% ... %] construction, to form the new alternatives.  The
core of the procedure, matching parts of the string with the RHSs of
rules, is as in our depth-first, bottom-up recognizer.  Notice how the
rewritten strings [^^left ^head ^^others] are simply left on the stack
to be collected up in the [% ... %] list.
